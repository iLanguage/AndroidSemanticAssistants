%% Semantic Assistants Documentation
%% 
%% This file is part of the Semantic Assistants architecture.
%%
%% Copyright (C) 2009, 2010, 2011 Semantic Software Lab, http://www.semanticsoftware.info
%%
%% The Semantic Assistants architecture is free software: you can
%% redistribute and/or modify it under the terms of the GNU Affero General
%% Public License as published by the Free Software Foundation, either
%% version 3 of the License, or (at your option) any later version.
%% 
%% This program is distributed in the hope that it will be useful,
%% but WITHOUT ANY WARRANTY; without even the implied warranty of
%% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%% GNU Affero General Public License for more details.
%% 
%% You should have received a copy of the GNU Affero General Public License
%% along with this program.  If not, see <http://www.gnu.org/licenses/>.
%%

\chapter{Introduction to Semantic Assistants}

\section{Overview}
The \sa project aims to bring natural language processing (NLP)
techniques directly to end users by integrating them with common
desktop applications, such as word processors, email clients, or Web
browsers. To facilitate the connection between NLP frameworks and
(desktop) clients, a service-oriented architecture
(Figure~\ref{fig:arch}) has been developed that allows to integrate
(desktop) clients with NLP services implemented in the GATE
framework.\footnote{GATE, \url{http://gate.ac.uk}} NLP services are
published as standard Web services with a WSDL description.

For the general motivation, design, and background on the \sa project,
please read the information on the \sa Web site\footnote{Semantic
  Assistants,
  \url{http://www.semanticsoftware.info/semantic-assistants}} and the
contained references first \citep{giwi08,aswc08}. This document only
describes the installation and use of the developed system.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{pictures/arch}
  \caption{The \sa architecture}
  \label{fig:arch}
\end{figure}

\section{How to read this documentation}
To deploy \sa, we recommend you first read this overview chapter.
Then, consult the following parts of this documentation:

\begin{description}
\item[End Users:] Please refer to the installation guide in
  Chapter~\ref{chap:inst}, as well as the client-specific installation
  instructions in Chapter~\ref{chap:clients}. Additionally, if you
  want to run your own server, read the server installation guide
  (Chapter~\ref{chap:serv}).

\item[Language Engineers:] If you want find out how to integrate a
  GATE pipeline as a new NLP service, please refer to
  Chapter~\ref{chap:services} for documentation on the OWL service
  descriptions and Section~\ref{sec:nlpservices} for publishing the
  new service through the \sa server.

\item[Plug-in Developers:] Please first read the background material
  (Web site, papers). Then refer to the developers' notes in
  Chapter~\ref{chap:dev}.
\end{description}

\section{Architectural Overview}
\label{sec:implementation}
This section gives an overview over the current version of the Semantic
Assistants architecture, as shown in Figure~\ref{fig:arch}:

\paragraph{Tier 1} of the architecture consists of client applications
and a \emph{Client-Side Abstraction Layer (CSAL)}. Currently, there
are two example clients distributed with the system, a simple
command-line client for testing purposes and a plug-in for the
OpenOffice.org \emph{Writer} word processor. The client-side abstraction
layer consists partly of hand-written Java classes that provide
common client-side functionality, partly of automatically generated
Java classes. The communication between client and server is
implemented by means of W3C Web services.\footnote{Web Services Architecture, see
  \url{http://www.w3.org/TR/ws-arch/}} 

\paragraph{Tier 2} of the architecture consists of a \emph{Web server}
and the \emph{NLP Service Connector}. The Web server used by default
in the architecture is the Java~6 embedded Web server.  The NLP
Service Connector currently integrates the GATE framework for NLP. It
is responsible for a number of tasks, including communication with the
client, reading and querying the language service descriptions,
running requested language services, and generating response messages.

\paragraph{Tier 3} is the NLP subsystem. At present, only the GATE
framework is supported (future work might integrate additional
frameworks, such as UIMA).  It makes use of the GATE API in order to
assemble language services, store them in a permanent way, and invoke
them when they are requested by a client.

\paragraph{Tier 4} is the resource tier.  Here we have the language
service descriptions, which are authored in the Web Ontology Language
(OWL).  Tier~4 further contains external documents, which the NLP
subsystem must be able to access. Finally, we possibly have
pre-indexed documents as part of the resources tier. For indexing,
GATE uses Apache's Lucene indexer\footnote{Apache Lucene, see
  \url{http://lucene.apache.org/}} as a subsystem, and allows us,
through its API, to create and access indices.


\section{System Components}
\label{sec:syscomp}
The implementation of the Semantic Assistants architecture currently comes
with the following components:

\begin{description}
\item[Server:] The server is the core of the architecture.  It
  communicates with the clients through the CSAL on one hand and
  the NLP framework through the NLP Service Connector on the other.
  At present, the architecture only contains a connector for
  GATE. However, it was explicitly designed to allow an easy
  integration of other frameworks (for example, UIMA). For describing
  available services, we use ontology-based (OWL) service
  descriptions. As a service-oriented architecture (SOA), every
  service is automatically available to all clients connected to the
  architecture, using standard WSDL\footnote{Web Services Description
    Language (WSDL), see \url{http://www.w3.org/TR/wsdl}}) interface
  descriptions.

\item[Client-Side Abstraction Layer (CSAL):] Our top goal was to make
  it as easy as possible for client (plug-in) developers to integrate
  NLP functionality. As clients should be able to connect to the
  architecture entirely by ``local'' means, we provide an
  \emph{abstraction layer}, named CSAL, which is located on the client
  side and performs the actual communication with the server.

  Apart from the communication functionality, the CSAL also provides
  common client-side functionality, i.e., useful data types and
  methods that are frequently required when integrating NLP into
  desktop clients.

\item[Clients:] Two example clients come with the architecture: a
  command-line client and a plug-in for the OpenOffice.org Writer word
  processor. 

\item[Example Resources:] NLP functionality is provided to clients
  through Web services. To match clients with suitable services
  (depending on language, formats, etc.), each NLP service comes with
  a semantic service description in OWL format. Three example service
  descriptions are included in the current distribution: an
  information extraction (IE) service that detects persons and locations
  (using GATE's ANNIE pipeline), an IR service (using the Yahoo PR)
  and a compound service, which combines the IR and the IE
  service. These should help you in defining your own NLP services
  that you deliver to your end users (e.g., summarization,
  question-answering, domain-specific NLP services).

\item[Documentation and Online Resources:] Apart from this guide, a
  number of publications on the \sa are available
  online,\footnote{\sa,
    \url{http://www.semanticsoftware.info/semantic-assistants}} as
  well as a discussion forum for support.
\end{description}


% \begin{figure}
%   \centering
%   %\vspace*{-9mm}
%   \includegraphics[width=0.6\textwidth]{pictures/abstraction.jpg}
%   \caption{We introduce a client-side abstraction layer}
%   \label{fig:abstraction}
%   %\vspace*{-0.4cm}
% \end{figure}

\section{Example NLP Services}
A number of example pipelines (NLP services) come with the architecture.
They are located in the \url{Resources} directory. There are two
parts: the semantic service descriptions in OWL format stored in the
\url{OwlServiceDescriptions} directory and corresponding GATE
pipelines (\url{.xgapp} files) implementing these services in the
\url{GatePipelines} directory.

\paragraph{Person and Location Extractor.} This NLP service runs the
ANNIE IE system that comes with the standard GATE distribution.  It
detects a number of named entities, such as persons, locations,
organizations, etc.  An OWL service description for this pipeline is
already implemented and stored in the directory
\url{Resources/OwlServiceDescriptions/annie.owl}.  For details on the
OWL-based description format, please refer to Section~\ref{sec:owl}.

\paragraph{Yahoo Search.} This service performs a Web search for a
user and returns the first $n$ documents (where $n$ is a
user-configurable runtime parameter). While this service usually works
as provided, you should obtain a Yahoo API key (see the
\href{http://gate.ac.uk/sale/tao/splitch19.html#x24-51700019.7}{GATE
  manual} for details on this) and store this key in the file
\url{Resources/GatePipelines/Yahoo/application.xgapp} by replacing the
string ``insertyahooidhere'':
\begin{verbatim}
    <string>applicationID</string>
    <string>insertyahooidhere</string>
\end{verbatim}

\paragraph{Web IR Extractor.} The third example service shows how to
combine two existing services, by first calling the Yahoo IR service
and then using the search results as input to the ANNIE IE
service. This service is located in \url{yahooExtractor.owl}.

\section{Service Output Types}
Following a successful execution of an NLP service, the generated results are sent to the clients in form of an XML document. Each result can have one of the following types:

\paragraph{Annotation} Annotation results are extracted entities that bear a \texttt{start} and \texttt{end} offset that represent the location of the annotation in their source document's text. Annotation instances are often highlighted in the text using their offsets or presented to users in lists, based on their implementation in various clients. In addition to offsets, annotation instances has \texttt{content} and \texttt{type} attributes that holds the annotation's textual content, i.e., the text between the two offsets, and their type. For example. the Person and Location Extractor service generates annotation instances of ``Person'' and ``Location'' types. Furthermore, Annotations hold additional information if they are provided with so-called \texttt{features}. Features are an unlimited list of additional information associated to an annotation in form of key and value pairs. For example, an annotation of type Person can have a \texttt{Gender} feature.
\paragraph{Boundless Annotation} Boundless annotations are special kind of annotations that adhere to the whole document and have no start and end offsets. Like Annotation instances, they may hold a list of features. The handling behaviour of such annotations are dependent of the clients implementations.  
\paragraph{File} Files generated by NLP services are stored on the server running the service and their identifiers along with some format information are merely passed to clients. Each file result has a \texttt{url}, a \texttt{mimeType} and \texttt{format} attribute representing a human-readable description of its mime type. The client can evaluate this information, and, if it decides to do so, request the result  file itself by using the identiÔ¨Åer sent to it. The server then sends the actual file and it is up to the client implementation on how to present it,

